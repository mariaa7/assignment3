I implemented this assignment using a web role, a worker role, and a class library. The web role contains the admin dashboard that holds functionality to start crawling the site, get the title from a URL, as well as deleting the data from the queue and table. Additionally, I used the web role to get the required data that needs to be on the dashboard including the state of the crawler, the machine counters, the total number of URLs crawled, the last ten urls crawled, how many urls are in the queue, how many URLs have been added to the table, and the URLs that are broken. 
The worker role checked the state of the crawler to see if it was idle, crawling, or needs to crawl the robots.txt. I did this through a cloud queue that once the crawler is started a new queue message is added and once it is done crawling another message is added that tells the worker role that it is crawling, as well as a stop message that prevents the worker role from running and clears all the data from the queue and table. In order to actually crawl robots, I did so in the class library through web request and stream reader, and for the cnn robots txt I used recursion. To crawl html pages I used html agility pack, checking if they are allowed, pulling all of the links, and adding them to the table in the worker role. I encoded the URL to use as the row key, used “partition” as the partition key if the link works, or “error” if it did not. In order to save the last ten urls I saved them to a list and after each loop through the while loop when crawling I inserted or updated them in the table. I saved the index number and #URLs in the table as well. 
Because of an error that I could not figure out, some of my project methods do not work on azure. However, you can still see dashboard and get title from url, etc.
